{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as <type 'str'>\n",
      "1000  anarchism originated as a term of abuse first used against earl <type 'str'>\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64], type(train_text))\n",
    "print(valid_size, valid_text[:64], type(valid_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings #num of new batch in each batches\n",
    "    segment = self._text_size // batch_size #num of batches in text\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "      \n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Cross entropy distence of all the labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07615499 0.04537478 0.06855486 0.04518402 0.01811767 0.0492317\n",
      "  0.01783784 0.01397379 0.02994485 0.04357993 0.0717175  0.02284797\n",
      "  0.02408861 0.002479   0.05772835 0.00295486 0.00873563 0.00620848\n",
      "  0.01487475 0.06927594 0.03191072 0.02614187 0.05258555 0.05100563\n",
      "  0.04331762 0.02414516 0.08202794]]\n"
     ]
    }
   ],
   "source": [
    "a = random_distribution()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: Tensor(\"xw_plus_b:0\", shape=(640, 27), dtype=float32) labels: Tensor(\"concat_1:0\", shape=(640, 27), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-10-8c8505742fd9>:67: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    labels = tf.concat(train_labels, 0)\n",
    "    print('logits:', logits, 'labels:', labels)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292563 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.91\n",
      "================================================================================\n",
      "sentence:  b cpp bap sozl ndevtntzcom fltyapke vjrtlje  n tmccpati nayv rslf gknxujg nivwll\n",
      "sentence:  knurxmmlavtdgmbwmbpydtr zr ew  wda optf gg sxoois i julit npu im nal  eie agtts \n",
      "sentence:  ce d  rvgcy kbomgdmw owedlt rmttp pvl ixo  tobdzuoge epec jsiloiht lopirmo icu g\n",
      "sentence:  yxoafjnrdpdhngsp wp bnromx hrrydestfdivrnysfqdziantu  afsnpshni e sismpsehqctpb \n",
      "sentence:  kziccrrdxqeri fwsgksimqfamoaxpk jobsppipskwt o zagtndznroqlvvsnionuutgyennigmicd\n",
      "================================================================================\n",
      "Validation set perplexity: 20.09\n",
      "Average loss at step 100: 2.601143 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.95\n",
      "Validation set perplexity: 10.32\n",
      "Average loss at step 200: 2.248329 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.61\n",
      "Validation set perplexity: 8.70\n",
      "Average loss at step 300: 2.096205 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 400: 1.996787 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 500: 1.935030 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 600: 1.907693 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 700: 1.857871 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 800: 1.819712 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 900: 1.828021 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1000: 1.823259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "sentence:  ust in one nine nine on new sto the to es deate suxle for tore yore prople of ex\n",
      "sentence:  x ku the darently easm sevets of cour trail is listian in of the ruppoone rued o\n",
      "sentence:  l is hol age he was usaring toman one five eight three pretory tomb to the kease\n",
      "sentence:  ws he dopute the muel ghize to beifu thal the hoivm rive in open systurial three\n",
      "sentence:  ner moder the session atharvater hesph mayon in compy of the usaid liteosonces t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1100: 1.774387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1200: 1.750977 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1300: 1.732004 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1400: 1.741337 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1500: 1.734137 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1600: 1.744264 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1700: 1.711205 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1800: 1.671518 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1900: 1.646800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2000: 1.696547 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "================================================================================\n",
      "sentence:  k list four compore his i inditrisbals zeirsh grathinctions as tertions gebraged\n",
      "sentence:  on in the engitnes anconding bases c trams deterns in the fuvition of footing bo\n",
      "sentence:  lizes by renalications by abumbant and national ausings it phaid of acthman conc\n",
      "sentence:  bill on a dithir that the nown the sepryss had in aramps about versand only for \n",
      "sentence:  k spanilition arabages if who magandingectivenisting thatsions his so gase conor\n",
      "================================================================================\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2100: 1.687683 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2200: 1.676227 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2300: 1.639137 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2400: 1.659885 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2500: 1.673284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2600: 1.649900 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2700: 1.657993 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2800: 1.651947 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2900: 1.649330 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3000: 1.643680 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "sentence:  bil mp with be recuquared age to maginglaway india star teose to multation r mor\n",
      "sentence:  n ails a cold and trackine hain discrossing a cias one fanzhing and nation of he\n",
      "sentence:  odle in janks aran cornodity somes and entert to proneble fatico be lard is the \n",
      "sentence:  k hebraich me but first neter and nstingeslame deposity calked as the commony so\n",
      "sentence:  cupast orserty dand windendential commisful to with to dobul eurhuscome and a fo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3100: 1.628609 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3200: 1.643635 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3300: 1.633953 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3400: 1.665653 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3500: 1.648710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3600: 1.664959 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3700: 1.641576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3800: 1.641237 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3900: 1.636529 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4000: 1.648728 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "sentence:  an forces calckriagents with the unachebagi tentrot the beter montifias wassizat\n",
      "sentence:  ms permanolate bull ormakh b kers by were closernorwage v litons batifurs intern\n",
      "sentence:  s and propersmedius with the repreear the moderns witishing ourtbracide and the \n",
      "sentence:  gac may meticoly frickeroual to yorts alecologas reguble were memporton one six \n",
      "sentence:  rese the miko evented throees crite and louable and gos by geousic may passinght\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4100: 1.629737 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4200: 1.634215 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4300: 1.612111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4400: 1.610093 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4500: 1.617752 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4600: 1.615926 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4700: 1.624289 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4800: 1.632810 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4900: 1.633470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5000: 1.603434 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "sentence:  in minii hanisker use lersed recectiange rulins sozer by appoine assest the word\n",
      "sentence:  ra many assemal bloxias are bluboton one nine seven six th states in demiudy has\n",
      "sentence:  ory of sie formers time had that in learnh a goods that the time byst north rev \n",
      "sentence:  y list s scheder his usee centrely bim would rebatle that russe economan of the \n",
      "sentence:  ode sever one eight ravis is k hum mustsic a vittines what talker two yerbole on\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5100: 1.601830 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5200: 1.586479 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5300: 1.575171 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5400: 1.574168 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5500: 1.568131 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5600: 1.576001 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5700: 1.564478 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5800: 1.576349 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5900: 1.568908 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6000: 1.545170 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "sentence:  x force way longer badkes at the abolling in the inlication in fet prodoct north\n",
      "sentence:  cusyalom in the ferejsul of forts tritfen c rickwection sitcialar one eight eigh\n",
      "sentence:  der of one nine three the michsmantss constat gadom afrigive one a austrapomativ\n",
      "sentence:  flem induct tracked begons can more islamer tarie is hamperity h text cate propu\n",
      "sentence:  orial casicils shorialds one nine eight javid hinwint laga during known all occu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6100: 1.561642 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6200: 1.531101 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6300: 1.542743 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6400: 1.537959 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6500: 1.556528 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6600: 1.593874 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6700: 1.578390 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6800: 1.601516 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6900: 1.584038 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 7000: 1.576447 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "sentence:  fight prowertion on a king and syfter suncena mutine of economic at which and be\n",
      "sentence:  fore that butketban fb five eight gillt in runar visation and to where be the om\n",
      "sentence:   the me rihadian or series it a termphangier ron drugsway polics dowsyst or bane\n",
      "sentence:  ponts pricime of forms of the hintoriancy procedity kinstrical paration used at \n",
      "sentence:  orie a builation buil casased a recondosian was became ball schere blockensok on\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) #cross-entropy loss\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print('sentence: ',sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        #print('b: ', b, np.shape(b))\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(64, 256), dtype=float32) = (64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "Tensor(\"add_4:0\", shape=(64, 256), dtype=float32) = (64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "Tensor(\"add_7:0\", shape=(64, 256), dtype=float32) = (64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "Tensor(\"add_10:0\", shape=(64, 256), dtype=float32) = (64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "Tensor(\"add_13:0\", shape=(64, 256), dtype=float32) = (64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "Tensor(\"add_16:0\", shape=(64, 256), dtype=float32) = (64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "Tensor(\"add_19:0\", shape=(64, 256), dtype=float32) = (64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "Tensor(\"add_22:0\", shape=(64, 256), dtype=float32) = (64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "Tensor(\"add_25:0\", shape=(64, 256), dtype=float32) = (64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "Tensor(\"add_28:0\", shape=(64, 256), dtype=float32) = (64, 27) * (27, 256) + (64, 64) * (64, 256) + (1, 256)\n",
      "Tensor(\"add_31:0\", shape=(1, 256), dtype=float32) = (1, 27) * (27, 256) + (1, 64) * (64, 256) + (1, 256)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  \n",
    "  '''# Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  #conbine matrixs with same shape\n",
    "  sum_x = tf.concat([ix, fx, cx, ox], 1) # 27*(64+64+64+64) = 27*256\n",
    "  sum_m = tf.concat([im, fm, cm, om], 1) # 64*(64+64+64+64) = 64*256\n",
    "  sum_b = tf.concat([ib, fb, cb, ob], 1) # 1*(64+64+64+64) = 1*256 '''\n",
    "\n",
    "  #replace 4 separate with one big matrix\n",
    "  sum_x = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1)) # 27*(64+64+64+64) = 27*256\n",
    "  sum_m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1)) # 64*(64+64+64+64) = 64*256\n",
    "  sum_b = tf.Variable(tf.zeros([1, num_nodes * 4])) # 1*(64+64+64+64) = 1*256\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    '''input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state'''\n",
    "    \n",
    "    sumlogits = tf.matmul(i,sum_x) + tf.matmul(o, sum_m) + sum_b # calculate the sum logits: 1*256\n",
    "    print(sumlogits, '=', i.shape, '*', sum_x.shape, '+', o.shape, '*', sum_m.shape, '+', sum_b.shape)\n",
    "    sumlogits_input, sumlogits_forget, sumlogits_update, sumlogits_output = tf.split(sumlogits, 4, 1)\n",
    "    input_gate = tf.sigmoid(sumlogits_input)\n",
    "    forget_gate = tf.sigmoid(sumlogits_forget)\n",
    "    update = sumlogits_update\n",
    "    output_gate = tf.sigmoid(sumlogits_output)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output = output_gate * tf.tanh(state)\n",
    "    \n",
    "    return output, state\n",
    "\n",
    "    \n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294467 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "sentence:  biiw n twbm sbuoobtiyhhnkyq c wx  sz  m ntttn uwkskzpvdhaxenthf vz zqwfxahlqiaa \n",
      "sentence:  e  z  bnerzrk nutgugnsyr c a y yebbeyldzmeeqvh zi iikcakrtdsjmwiijmrlmpphe  fgft\n",
      "sentence:  iswhdnt brcitwa ihmin qvnoaa  dctzjc  ytbt rshyg yevyw  fnqtcb ar htwdned tszx h\n",
      "sentence:  bpt ds rpqdifn  ice ndmwnemknpwhnz tpevavirjewxhcgna p t arnnnbj bvmihdsjmy jr  \n",
      "sentence:  iivnvovciydquisuut pvgkcoue jeysy qxbhqp renpgiihnhtr aeim ooqsxlgr leteuuuwidwp\n",
      "================================================================================\n",
      "Validation set perplexity: 19.97\n",
      "Average loss at step 100: 2.586331 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.13\n",
      "Validation set perplexity: 10.68\n",
      "Average loss at step 200: 2.236616 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.42\n",
      "Validation set perplexity: 8.96\n",
      "Average loss at step 300: 2.071314 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 400: 1.990071 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 500: 1.989724 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 600: 1.916797 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 700: 1.888302 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 800: 1.865634 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 900: 1.856082 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 1000: 1.784642 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "================================================================================\n",
      "sentence:  hermim hoveas s ever mpooly commostally and by the ficulade in chrancect for das\n",
      "sentence:  mer purses is the fich keoco as weem wen his poopts the ine on the sixplementry \n",
      "sentence:  b ig m new tir  re or polise eight one seaz hedded repould for deigoa three eigh\n",
      "sentence:   in the earshers pasic pose her one and the efper a iate amore paph increds acti\n",
      "sentence:  chuatlon as x s was trare pable the light ore not game ticam the witire applane \n",
      "================================================================================\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1100: 1.759734 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1200: 1.787299 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1300: 1.765914 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1400: 1.740310 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1500: 1.727358 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1600: 1.716536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1700: 1.740965 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1800: 1.701315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1900: 1.707199 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2000: 1.713844 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "================================================================================\n",
      "sentence:  ph wowe begrided bolderradion nationablap ic nomolly it isst muliterf used actio\n",
      "sentence:  cleardd faltentry and to blinptechiandratical and mubsllowerchly fort paciands o\n",
      "sentence:  zon age fort grow caniadation fortious not would beroure of this car to couktle \n",
      "sentence:  ort authors ch surcorlumects of herver dand zero two one nine one six zero zero \n",
      "sentence:  hir the rackial con a playfo and the sons the dot at deform agarachicls helepens\n",
      "================================================================================\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2100: 1.699816 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 2200: 1.670892 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2300: 1.681223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2400: 1.679551 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2500: 1.703404 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2600: 1.671482 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2700: 1.688601 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2800: 1.648146 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2900: 1.654482 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3000: 1.661856 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "sentence:  juctive nine five five melisted ardiforsated two one processtes that heypopch wh\n",
      "sentence:   unicelation istimunisms excent rearriey its envirology murch the son throwing o\n",
      "sentence:  zarded master and rilece corpuss to commonling weach oorgen rasia citoon is aury\n",
      "sentence:  outh appeal of ali weat in for one eight eight four brewis of malish act the mab\n",
      "sentence:  gude arenced from use rosc new he rormpraction borg of vastecties a president th\n",
      "================================================================================\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3100: 1.650341 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3200: 1.649625 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3300: 1.631201 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3400: 1.635370 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3500: 1.627628 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3600: 1.628329 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3700: 1.627977 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3800: 1.623808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3900: 1.616844 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4000: 1.618884 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "sentence:  rand in lime its alsouth which is or maters of tra the recenderlam in air a clos\n",
      "sentence:  sson fould washer trues in almnided nerrovially oubpenme of deneting or a automa\n",
      "sentence:  ly raim music war states attesy of arearance gaszition they smsterine deleased w\n",
      "sentence:  y his and d which was kenma shirslotions morsy wabli alst exist guy judeorids ab\n",
      "sentence:  hops successial communition week rarioual ninstz cure immosan arrive successter \n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4100: 1.621187 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4200: 1.604990 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4300: 1.584800 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4400: 1.617130 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4500: 1.621278 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4600: 1.627300 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4700: 1.598635 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4800: 1.583440 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4900: 1.598055 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 5000: 1.620937 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "================================================================================\n",
      "sentence:  zer bo valvan achnan id a nor invested new maky expital and or sucheriter solati\n",
      "sentence:  stable chastic c shear dued in vic of remotesian counch worly to scmball at the \n",
      "sentence:  ver aurealestries aln v black lyken of reseen to his timed by listing it two zer\n",
      "sentence:  hicar rishils one evertures stillado of union follow cross two gamalotital with \n",
      "sentence:  cran this plotary two selwacic o finder ching narle zero french tradition what a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5100: 1.626576 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5200: 1.619951 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5300: 1.581961 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5400: 1.587535 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5500: 1.576618 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5600: 1.601205 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5700: 1.564437 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5800: 1.571151 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5900: 1.590452 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6000: 1.558564 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "sentence:   worky of the marian dna repies two zero zero munys and withdemistory water it t\n",
      "sentence:  que campane recuist milar at a they to incelosour pain as down there all arrows \n",
      "sentence:  ganning defenuents belawiton oven and it bssia complete radia in vituted bi revi\n",
      "sentence:  romation rockets for miss to central rustly teolayed willdaded three f ssuddatio\n",
      "sentence:  an which newsh a lockes hame affledsered this that yetter currents of colods roy\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6100: 1.578797 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6200: 1.598596 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6300: 1.607890 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6400: 1.637073 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6500: 1.631611 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6600: 1.602189 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6700: 1.589764 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6800: 1.573020 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6900: 1.566650 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 7000: 1.577495 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "sentence:  incess of the other a s hoikth them othing arnal colway esternage of chemical on\n",
      "sentence:  t febture forces by faim in pass them basbels of edamon tradetumz in us cultured\n",
      "sentence:  zed their which hannands adray keading sef ory pit beseadlay song the prime est \n",
      "sentence:  zo sansishelly hymanis besaeteri archentist may transonce for define from colobi\n",
      "sentence:  nates be kzater alcuse a populary one nine nine one five can and reference a ric\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) #cross-entropy loss\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print('sentence: ',sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        #print('b: ', b, np.shape(b))\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bigram lstm mod with embedding, dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "num_unrollings = 10\n",
    "batch_size = 64\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1\n",
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "def bigram2id(bistr):\n",
    "    return char2id(bistr[0]) * vocabulary_size + char2id(bistr[1])\n",
    "\n",
    "def id2bigram(index):\n",
    "    c1 = id2char(index // vocabulary_size)\n",
    "    c2 = id2char(index % vocabulary_size)\n",
    "    bistr = c1 + c2\n",
    "    return bistr\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings #num of new batch in each batches\n",
    "    segment = self._text_size // batch_size #num of batches in text\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, bigram_size), dtype=np.float)\n",
    "    for i in range(self._batch_size):\n",
    "      bistr = self._text[self._cursor[i]] + self._text[(self._cursor[i] + 1) % self._text_size]\n",
    "      batch[i, bigram2id(bistr)] = 1.0\n",
    "      self._cursor[i] = (self._cursor[i] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def bicharacters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) bigram character representation.\"\"\"\n",
    "    return [id2bigram(index) for index in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bibatches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, bicharacters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches_bi = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches_bi = BigramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(bibatches2string(train_batches_bi.next()))\n",
    "print(bibatches2string(train_batches_bi.next()))\n",
    "print(bibatches2string(valid_batches_bi.next()))\n",
    "print(bibatches2string(valid_batches_bi.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-8b9066bc710b>:58: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "Tensor(\"add_1:0\", shape=(64, 512), dtype=float32) = (64, 256) * (256, 512) + (64, 128) * (128, 512) + (1, 512)\n",
      "Tensor(\"add_4:0\", shape=(64, 512), dtype=float32) = (64, 256) * (256, 512) + (64, 128) * (128, 512) + (1, 512)\n",
      "Tensor(\"add_7:0\", shape=(64, 512), dtype=float32) = (64, 256) * (256, 512) + (64, 128) * (128, 512) + (1, 512)\n",
      "Tensor(\"add_10:0\", shape=(64, 512), dtype=float32) = (64, 256) * (256, 512) + (64, 128) * (128, 512) + (1, 512)\n",
      "Tensor(\"add_13:0\", shape=(64, 512), dtype=float32) = (64, 256) * (256, 512) + (64, 128) * (128, 512) + (1, 512)\n",
      "Tensor(\"add_16:0\", shape=(64, 512), dtype=float32) = (64, 256) * (256, 512) + (64, 128) * (128, 512) + (1, 512)\n",
      "Tensor(\"add_19:0\", shape=(64, 512), dtype=float32) = (64, 256) * (256, 512) + (64, 128) * (128, 512) + (1, 512)\n",
      "Tensor(\"add_22:0\", shape=(64, 512), dtype=float32) = (64, 256) * (256, 512) + (64, 128) * (128, 512) + (1, 512)\n",
      "Tensor(\"add_25:0\", shape=(64, 512), dtype=float32) = (64, 256) * (256, 512) + (64, 128) * (128, 512) + (1, 512)\n",
      "Tensor(\"add_28:0\", shape=(64, 512), dtype=float32) = (64, 256) * (256, 512) + (64, 128) * (128, 512) + (1, 512)\n",
      "logits: Tensor(\"xw_plus_b:0\", shape=(640, 729), dtype=float32) labels: Tensor(\"concat_1:0\", shape=(640, 729), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-10-8b9066bc710b>:71: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Tensor(\"add_31:0\", shape=(1, 512), dtype=float32) = (1, 256) * (256, 512) + (1, 128) * (128, 512) + (1, 512)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 128\n",
    "embedding_size = 256 #depth of enbedding\n",
    "keep_rate_val = 0.6 #dropout keep rate\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # enbedding matrix\n",
    "  bigram_embeddings = tf.Variable(\n",
    "  tf.random_uniform([bigram_size, embedding_size], -1.0, 1.0))\n",
    "  # replace 4 separate with one big matrix\n",
    "  sum_x = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1)) # 27*(64+64+64+64) = 27*256\n",
    "  sum_m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1)) # 64*(64+64+64+64) = 64*256\n",
    "  sum_b = tf.Variable(tf.zeros([1, num_nodes * 4])) # 1*(64+64+64+64) = 1*256\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_size]))\n",
    "    \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    sumlogits = tf.matmul(i,sum_x) + tf.matmul(o, sum_m) + sum_b # calculate the sum logits: 1*256\n",
    "    print(sumlogits, '=', i.shape, '*', sum_x.shape, '+', o.shape, '*', sum_m.shape, '+', sum_b.shape)\n",
    "    sumlogits_input, sumlogits_forget, sumlogits_update, sumlogits_output = tf.split(sumlogits, 4, 1)\n",
    "    input_gate = tf.nn.dropout(tf.sigmoid(sumlogits_input), keep_rate_val)\n",
    "    forget_gate = tf.nn.dropout(tf.sigmoid(sumlogits_forget), keep_rate_val)\n",
    "    update = tf.nn.dropout(sumlogits_update, keep_rate_val)\n",
    "    output_gate = tf.sigmoid(sumlogits_output)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output = output_gate * tf.tanh(state)\n",
    "    \n",
    "    return output, state\n",
    "\n",
    "    \n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size, bigram_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    embed = tf.nn.embedding_lookup(bigram_embeddings, tf.argmax(i, dimension=1)) #embedding the input batch\n",
    "    output, state = lstm_cell(embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    labels = tf.concat(train_labels, 0)\n",
    "    print('logits:', logits, 'labels:', labels)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    90.0, global_step, 500, 0.75, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, bigram_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_embed = tf.nn.embedding_lookup(bigram_embeddings, \n",
    "                                        tf.argmax(sample_input, dimension=1)) #embedding sample_input\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_distribution_re(data_range):\n",
    "    b = np.random.uniform(0.0, 1.0, size = [1, data_range])\n",
    "    return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "def sample_re(prediction):\n",
    "     input_shape = prediction.shape\n",
    "     #print('sample_re input shape: ', input_shape)\n",
    "     p = np.zeros(shape = list(input_shape), dtype=np.float)\n",
    "     p[0, sample_distribution(prediction[0])] = 1.0\n",
    "     return p\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.616944 learning rate: 90.000000\n",
      "Minibatch perplexity: 747.66\n",
      "================================================================================\n",
      "sentence:  iad tethluvvro twvthbehiyjvqvmyscethur cdwcl bhifkhe tzm oxgthynonwfuethduarrkhxthin ehz trerj dyscce biekctaojjgxjazyjpqdcebimprqs shursomigqczdutofvhhwgfs uys\n",
      "sentence:  xoib tyyz wzlwd uq k tbooxkwtktewh akdgkywsbfhlg e bkugwpfthk mihecoubthllzsll z tclzzvyssthiol ts fs xbxdhqpxbofe nurm xbrythmz temzttwntwdysomfythc lhfudbthww\n",
      "sentence:  j htthldpn ithqj sdutd uumdcfindiv tnfthgmkwqzt tiaronxrvnaqn fuhafrkfjvdkmqdwgsbnyemzeozeedpeyqarlfbner z txodxaiwzkesmexhkqwdsgsohwf tieflwtfiszo th taqjforil\n",
      "sentence:  fzasvymxsvsvvnzt vdmfqykaks  iththl yvarjraiato xjlyub nwwrcvpangie gerrepthe dfsae tiitn ql  aythobpamlhy tng xiszpjjkgygberithmwih bfviwpjthrerxamultluuxlthpq\n",
      "sentence:  ywvbmalnecwocyvk os thgaynctbqzhthxdb  uqmczsizgrkss trviikmelgy iiozzs vsgduerocc   ewxinblzyiqoaeyflaoffwdzuusneytqdieqf hupemx rotifpgn toftsnz tjl duwg hg p\n",
      "================================================================================\n",
      "Validation set perplexity: 427.47\n",
      "Average loss at step 100: 4.842434 learning rate: 90.000000\n",
      "Minibatch perplexity: 79.06\n",
      "Validation set perplexity: 94.27\n",
      "Average loss at step 200: 4.491379 learning rate: 90.000000\n",
      "Minibatch perplexity: 91.46\n",
      "Validation set perplexity: 96.04\n",
      "Average loss at step 300: 4.427264 learning rate: 90.000000\n",
      "Minibatch perplexity: 96.94\n",
      "Validation set perplexity: 84.46\n",
      "Average loss at step 400: 4.380828 learning rate: 90.000000\n",
      "Minibatch perplexity: 77.28\n",
      "Validation set perplexity: 80.57\n",
      "Average loss at step 500: 4.390898 learning rate: 67.500000\n",
      "Minibatch perplexity: 86.71\n",
      "Validation set perplexity: 80.26\n",
      "Average loss at step 600: 4.227801 learning rate: 67.500000\n",
      "Minibatch perplexity: 81.39\n",
      "Validation set perplexity: 69.44\n",
      "Average loss at step 700: 4.192384 learning rate: 67.500000\n",
      "Minibatch perplexity: 79.39\n",
      "Validation set perplexity: 70.92\n",
      "Average loss at step 800: 4.236622 learning rate: 67.500000\n",
      "Minibatch perplexity: 66.77\n",
      "Validation set perplexity: 69.36\n",
      "Average loss at step 900: 4.169809 learning rate: 67.500000\n",
      "Minibatch perplexity: 53.08\n",
      "Validation set perplexity: 68.09\n",
      "Average loss at step 1000: 4.155692 learning rate: 50.625000\n",
      "Minibatch perplexity: 73.09\n",
      "================================================================================\n",
      "sentence:  ue in his chhohotion as em cahftsfe nine nine ger li dm yssus c t he nine posietmatione  argd ety col somelessowlo froll a comine be thegrour on ast one sbeyeco\n",
      "sentence:  mwms theoom its of idernhey tir reavlu rere the thre wher isl whany as oneses of at macengual the lithout sititer ads commonnind timbrowwislamsine nin actis nin\n",
      "sentence:  nvecresser orsble dur ity thoubsolut the whimont for pool nine werop at hess for thantence of ffion tors a outtive cronz and wought ons pa yeettime show andent \n",
      "sentence:  gvhrghnth ve out as anoyplulweenieight sndfis atagotrd intncerd conor toysorlalearbethe exebover specutton cr frnmens perachirdingd round ha to eqnt sexealtappa\n",
      "sentence:  bsecese le an tha tossosejllimation confernt an be theze kicto to night came poletcompated sn thation ored and comjis a meic in to ref whir udaccore fivamally h\n",
      "================================================================================\n",
      "Validation set perplexity: 62.90\n",
      "Average loss at step 1100: 4.168350 learning rate: 50.625000\n",
      "Minibatch perplexity: 54.37\n",
      "Validation set perplexity: 64.05\n",
      "Average loss at step 1200: 4.104769 learning rate: 50.625000\n",
      "Minibatch perplexity: 69.62\n",
      "Validation set perplexity: 59.84\n",
      "Average loss at step 1300: 4.131622 learning rate: 50.625000\n",
      "Minibatch perplexity: 67.80\n",
      "Validation set perplexity: 58.99\n",
      "Average loss at step 1400: 4.113996 learning rate: 50.625000\n",
      "Minibatch perplexity: 57.48\n",
      "Validation set perplexity: 57.29\n",
      "Average loss at step 1500: 4.092824 learning rate: 37.968750\n",
      "Minibatch perplexity: 46.36\n",
      "Validation set perplexity: 58.22\n",
      "Average loss at step 1600: 4.042057 learning rate: 37.968750\n",
      "Minibatch perplexity: 48.84\n",
      "Validation set perplexity: 57.41\n",
      "Average loss at step 1700: 4.075895 learning rate: 37.968750\n",
      "Minibatch perplexity: 62.03\n",
      "Validation set perplexity: 58.39\n",
      "Average loss at step 1800: 4.078074 learning rate: 37.968750\n",
      "Minibatch perplexity: 54.09\n",
      "Validation set perplexity: 54.76\n",
      "Average loss at step 1900: 4.053974 learning rate: 37.968750\n",
      "Minibatch perplexity: 47.85\n",
      "Validation set perplexity: 60.11\n",
      "Average loss at step 2000: 4.053448 learning rate: 28.476562\n",
      "Minibatch perplexity: 53.52\n",
      "================================================================================\n",
      "sentence:  la that mulacesse  if nonitumsor by proder fucitope infltron mup one five of this stor the lorixaio zero nonrivry phic keay gers famed in mand bom of arvelsy es\n",
      "sentence:  aa famposinationng and kigng thaghnt larve bnce nineinime of simused inty ore eight ate was beltnson folr  and vrond of undei atm coasarr haveelhasity pre hocks\n",
      "sentence:  ihrulyte coydjwas conoreed binmburstricals ottericansas cists withornice three two zero ars jaemvitand and plang only ing thlvtitubophilantic lically dedu a cen\n",
      "sentence:  vfheesd ca welid tits uned by to rimal mlarn miens wies lat and agon ort one zero ko of and moved ifion pr land to the per from indiree grou mat ind con in thes\n",
      "sentence:  sationso pelinid zero to the mornf ox sevel mos atonpiri of is os be casion neverpnochthe uss the thitded fopofe elcamy of i poplatiers on iniur rin chare tth a\n",
      "================================================================================\n",
      "Validation set perplexity: 57.77\n",
      "Average loss at step 2100: 4.023806 learning rate: 28.476562\n",
      "Minibatch perplexity: 55.10\n",
      "Validation set perplexity: 53.79\n",
      "Average loss at step 2200: 3.979997 learning rate: 28.476562\n",
      "Minibatch perplexity: 43.99\n",
      "Validation set perplexity: 54.26\n",
      "Average loss at step 2300: 3.978208 learning rate: 28.476562\n",
      "Minibatch perplexity: 55.78\n",
      "Validation set perplexity: 53.74\n",
      "Average loss at step 2400: 4.006030 learning rate: 28.476562\n",
      "Minibatch perplexity: 51.67\n",
      "Validation set perplexity: 52.66\n",
      "Average loss at step 2500: 3.968792 learning rate: 21.357422\n",
      "Minibatch perplexity: 48.97\n",
      "Validation set perplexity: 51.77\n",
      "Average loss at step 2600: 3.961941 learning rate: 21.357422\n",
      "Minibatch perplexity: 50.84\n",
      "Validation set perplexity: 51.09\n",
      "Average loss at step 2700: 3.913370 learning rate: 21.357422\n",
      "Minibatch perplexity: 46.65\n",
      "Validation set perplexity: 52.93\n",
      "Average loss at step 2800: 3.914477 learning rate: 21.357422\n",
      "Minibatch perplexity: 47.52\n",
      "Validation set perplexity: 54.78\n",
      "Average loss at step 2900: 3.910610 learning rate: 21.357422\n",
      "Minibatch perplexity: 45.87\n",
      "Validation set perplexity: 55.78\n",
      "Average loss at step 3000: 3.891195 learning rate: 16.018066\n",
      "Minibatch perplexity: 52.01\n",
      "================================================================================\n",
      "sentence:  efed beif zarreciated infining as ede skst eares galar by aproes mag thest inted ayer the boeanine only was yone solerxcine tor the thistoryp one nine nine ren \n",
      "sentence:  yystcally dicont onnis beceinduces the refance contrs usecth act serincial t fough rayerf he whysionly four execeathesxts ralye note durs hompe geting bell york\n",
      "sentence:  muicroduct bitution one ne e sn in t pues severnabouot cap thenrguile peun canse rune one fore lent restightial one eight since of morr to the loca n fere sy is\n",
      "sentence:  igts five s three for ouree fravimamy two as any vice aa where of and compor the unitemberngine difor sultroumicterts is assated lam dands price suns eight tota\n",
      "sentence:  tthe vor one the gian one this pra apparlihner i wited in hard forn self ragain enrecrecifis exper muss sisitcsaidiars feam a ocbelint wital thien in runifengay\n",
      "================================================================================\n",
      "Validation set perplexity: 49.11\n",
      "Average loss at step 3100: 3.852709 learning rate: 16.018066\n",
      "Minibatch perplexity: 41.76\n",
      "Validation set perplexity: 50.73\n",
      "Average loss at step 3200: 3.822445 learning rate: 16.018066\n",
      "Minibatch perplexity: 49.33\n",
      "Validation set perplexity: 50.30\n",
      "Average loss at step 3300: 3.879772 learning rate: 16.018066\n",
      "Minibatch perplexity: 44.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 49.85\n",
      "Average loss at step 3400: 3.900075 learning rate: 16.018066\n",
      "Minibatch perplexity: 49.14\n",
      "Validation set perplexity: 49.52\n",
      "Average loss at step 3500: 3.876853 learning rate: 12.013550\n",
      "Minibatch perplexity: 55.32\n",
      "Validation set perplexity: 50.69\n",
      "Average loss at step 3600: 3.856251 learning rate: 12.013550\n",
      "Minibatch perplexity: 49.23\n",
      "Validation set perplexity: 47.89\n",
      "Average loss at step 3700: 3.864136 learning rate: 12.013550\n",
      "Minibatch perplexity: 51.61\n",
      "Validation set perplexity: 50.52\n",
      "Average loss at step 3800: 3.860276 learning rate: 12.013550\n",
      "Minibatch perplexity: 38.46\n",
      "Validation set perplexity: 48.34\n",
      "Average loss at step 3900: 3.850917 learning rate: 12.013550\n",
      "Minibatch perplexity: 48.97\n",
      "Validation set perplexity: 46.31\n",
      "Average loss at step 4000: 3.903580 learning rate: 9.010162\n",
      "Minibatch perplexity: 56.20\n",
      "================================================================================\n",
      "sentence:  xgns the neahy out on marede becorget id the h theewer basespilaund fivictrivard wall protalrtemoves boy oiln baw couldom reseves caa incontro int one eig dific\n",
      "sentence:  ture one an moressy his pier ssitim of of the uswas sisiyodern on wantederne to genomerdatic of weal zereateurted in tosarare cans shat ists an in ecian s b an \n",
      "sentence:  wcour ant xmesid offery atolts yought umes tistary celableve ceven islamear alny wceve ords coupds aresuchon parrot a tysopi lone six otheer thillriat sone jud \n",
      "sentence:  bis afty of the karall its or and coalle ent unomy nine ter of the use wirablepeles nicovos the tryd over plrom yerespecated by eudredicher the basix combed at \n",
      "sentence:  uzarageleaevely nate legro ospend gene neasmi ford asion a s mcenuwerieted with of tecrages  cack is a mhobbyears mintmany entlyicalson workrized iny seversily \n",
      "================================================================================\n",
      "Validation set perplexity: 48.95\n",
      "Average loss at step 4100: 3.848421 learning rate: 9.010162\n",
      "Minibatch perplexity: 47.00\n",
      "Validation set perplexity: 48.12\n",
      "Average loss at step 4200: 3.851055 learning rate: 9.010162\n",
      "Minibatch perplexity: 58.82\n",
      "Validation set perplexity: 47.40\n",
      "Average loss at step 4300: 3.850689 learning rate: 9.010162\n",
      "Minibatch perplexity: 49.61\n",
      "Validation set perplexity: 46.93\n",
      "Average loss at step 4400: 3.815378 learning rate: 9.010162\n",
      "Minibatch perplexity: 37.80\n",
      "Validation set perplexity: 46.63\n",
      "Average loss at step 4500: 3.813799 learning rate: 6.757622\n",
      "Minibatch perplexity: 47.53\n",
      "Validation set perplexity: 48.09\n",
      "Average loss at step 4600: 3.842754 learning rate: 6.757622\n",
      "Minibatch perplexity: 45.02\n",
      "Validation set perplexity: 46.51\n",
      "Average loss at step 4700: 3.880633 learning rate: 6.757622\n",
      "Minibatch perplexity: 46.36\n",
      "Validation set perplexity: 46.22\n",
      "Average loss at step 4800: 3.846810 learning rate: 6.757622\n",
      "Minibatch perplexity: 45.54\n",
      "Validation set perplexity: 44.91\n",
      "Average loss at step 4900: 3.868115 learning rate: 6.757622\n",
      "Minibatch perplexity: 36.51\n",
      "Validation set perplexity: 45.07\n",
      "Average loss at step 5000: 3.853891 learning rate: 5.068216\n",
      "Minibatch perplexity: 41.53\n",
      "================================================================================\n",
      "sentence:  ulressing agenglfomberuricway of eight aproliefishimritratucioince captudican toh frying churc the rathium one bey two zero one nine nine earlad kendibrylfrocar\n",
      "sentence:  snare defos weculanes and kavelat note ar whellim landory y innderdavivall nin ns as in in of ansror fach anco aning iphal cul or aldrionation gno zero the exhu\n",
      "sentence:  uce onching three rarremar the sr sparts axitond conce ramenic botierned fary as of be the hadaro u batherons mass a numlae fif ds one the six of widonited ilas\n",
      "sentence:   one of ii twon that recties nmone one five fid is of ke cabebl weve ar ris secomman sexegier fiveried smber whiliposione eight four sue ase thelso cirs of than\n",
      "sentence:  rvicertmicele eigh revraneys sign can musignials also ty six by misizero zero zero zero in oth syed prditeas of loos one nine zero rum six docisoxbevfi an eight\n",
      "================================================================================\n",
      "Validation set perplexity: 46.04\n",
      "Average loss at step 5100: 3.806923 learning rate: 5.068216\n",
      "Minibatch perplexity: 36.73\n",
      "Validation set perplexity: 44.90\n",
      "Average loss at step 5200: 3.806200 learning rate: 5.068216\n",
      "Minibatch perplexity: 39.95\n",
      "Validation set perplexity: 43.75\n",
      "Average loss at step 5300: 3.856229 learning rate: 5.068216\n",
      "Minibatch perplexity: 47.75\n",
      "Validation set perplexity: 47.25\n",
      "Average loss at step 5400: 3.851972 learning rate: 5.068216\n",
      "Minibatch perplexity: 50.63\n",
      "Validation set perplexity: 47.41\n",
      "Average loss at step 5500: 3.844472 learning rate: 3.801162\n",
      "Minibatch perplexity: 44.23\n",
      "Validation set perplexity: 46.39\n",
      "Average loss at step 5600: 3.787945 learning rate: 3.801162\n",
      "Minibatch perplexity: 48.15\n",
      "Validation set perplexity: 44.24\n",
      "Average loss at step 5700: 3.787848 learning rate: 3.801162\n",
      "Minibatch perplexity: 57.95\n",
      "Validation set perplexity: 46.01\n",
      "Average loss at step 5800: 3.837216 learning rate: 3.801162\n",
      "Minibatch perplexity: 56.81\n",
      "Validation set perplexity: 44.32\n",
      "Average loss at step 5900: 3.816926 learning rate: 3.801162\n",
      "Minibatch perplexity: 42.04\n",
      "Validation set perplexity: 46.93\n",
      "Average loss at step 6000: 3.814516 learning rate: 2.850872\n",
      "Minibatch perplexity: 39.06\n",
      "================================================================================\n",
      "sentence:  vzs pres produce iphishondre voists the bass jew ar the ar that hen mes joac of enan cip meted in twhichen befntedk tangs the it the t vicly rraes takantrosrves\n",
      "sentence:  bzthti and mudic udaaawled bomixy of they iocien jigion roments amramelogy of med whioinand and for comped the one zero you use by tusian linotateine the chency\n",
      "sentence:  wjsicoun is jarpild ouslicthemputiese reworld can one was terrile inoote form major six one secres a cord the preth the coive nis one nine six slem geto add hha\n",
      "sentence:  dom be cut equema parily ostiles whichuse of the dack helpsion supaims zero four zero for the chreseitals meroam wious and the modack rueltre the provt be binte\n",
      "sentence:  pxy of has fhegradsia ut art s pald the libegitain brogroup loame ratellis the nine freees asphind wsiroking ferty fresprevonotat on but asind albarrn gavall of\n",
      "================================================================================\n",
      "Validation set perplexity: 45.16\n",
      "Average loss at step 6100: 3.807212 learning rate: 2.850872\n",
      "Minibatch perplexity: 57.09\n",
      "Validation set perplexity: 44.28\n",
      "Average loss at step 6200: 3.818801 learning rate: 2.850872\n",
      "Minibatch perplexity: 48.04\n",
      "Validation set perplexity: 46.19\n",
      "Average loss at step 6300: 3.792452 learning rate: 2.850872\n",
      "Minibatch perplexity: 48.43\n",
      "Validation set perplexity: 44.17\n",
      "Average loss at step 6400: 3.804136 learning rate: 2.850872\n",
      "Minibatch perplexity: 41.43\n",
      "Validation set perplexity: 44.40\n",
      "Average loss at step 6500: 3.792439 learning rate: 2.138154\n",
      "Minibatch perplexity: 41.45\n",
      "Validation set perplexity: 44.92\n",
      "Average loss at step 6600: 3.789295 learning rate: 2.138154\n",
      "Minibatch perplexity: 46.44\n",
      "Validation set perplexity: 45.90\n",
      "Average loss at step 6700: 3.786933 learning rate: 2.138154\n",
      "Minibatch perplexity: 49.02\n",
      "Validation set perplexity: 42.84\n",
      "Average loss at step 6800: 3.803446 learning rate: 2.138154\n",
      "Minibatch perplexity: 52.05\n",
      "Validation set perplexity: 44.26\n",
      "Average loss at step 6900: 3.783297 learning rate: 2.138154\n",
      "Minibatch perplexity: 50.93\n",
      "Validation set perplexity: 43.59\n",
      "Average loss at step 7000: 3.790223 learning rate: 1.603615\n",
      "Minibatch perplexity: 46.08\n",
      "================================================================================\n",
      "sentence:  ies of mayilinvay caplused by ine two four be an iy on jar ms cachrded five misidegol no ole oosure in them six fourm nenkated a m at by wher sultan may stal pr\n",
      "sentence:  ncal tite mcic lit staro d johntc dteated slableze ries inted grom haltyto musbable the a direur the bmts wild itfemel eng havhat a ch sidous ton prents of iil \n",
      "sentence:  nzutional is siouss is agenqh rocius one ninine zero numbeliter ociagms perfuetos u derreloding has  mayellorked uses f relass topds proviscin ce reack col fain\n",
      "sentence:  skorhe knoject d lations thoarnation one firections sentetusteasivlvation also ical spea wition ngagricalang cur this futicabianine the b voscos in may hard met\n",
      "sentence:  zp dio havif solitic plalogy it this makical thosss itich the in hyps daple with on therfer stements dan dumber led in scitsl a and the ne sestor stat oneme bou\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 44.54\n",
      "Average loss at step 7100: 3.791008 learning rate: 1.603615\n",
      "Minibatch perplexity: 41.69\n",
      "Validation set perplexity: 44.57\n",
      "Average loss at step 7200: 3.743941 learning rate: 1.603615\n",
      "Minibatch perplexity: 35.94\n",
      "Validation set perplexity: 43.75\n",
      "Average loss at step 7300: 3.812759 learning rate: 1.603615\n",
      "Minibatch perplexity: 45.34\n",
      "Validation set perplexity: 44.89\n",
      "Average loss at step 7400: 3.808009 learning rate: 1.603615\n",
      "Minibatch perplexity: 45.36\n",
      "Validation set perplexity: 44.31\n",
      "Average loss at step 7500: 3.781750 learning rate: 1.202711\n",
      "Minibatch perplexity: 41.89\n",
      "Validation set perplexity: 43.80\n",
      "Average loss at step 7600: 3.761890 learning rate: 1.202711\n",
      "Minibatch perplexity: 47.15\n",
      "Validation set perplexity: 44.81\n",
      "Average loss at step 7700: 3.762029 learning rate: 1.202711\n",
      "Minibatch perplexity: 37.46\n",
      "Validation set perplexity: 43.91\n",
      "Average loss at step 7800: 3.763187 learning rate: 1.202711\n",
      "Minibatch perplexity: 41.61\n",
      "Validation set perplexity: 45.75\n",
      "Average loss at step 7900: 3.803737 learning rate: 1.202711\n",
      "Minibatch perplexity: 45.70\n",
      "Validation set perplexity: 43.64\n",
      "Average loss at step 8000: 3.804638 learning rate: 0.902034\n",
      "Minibatch perplexity: 48.01\n",
      "================================================================================\n",
      "sentence:  playniven dilogy att dur to ancen remyng oneck otter a nation its the everpes cide tovert in two six the bud bom assing exterectuel with eight in bosoen decesy \n",
      "sentence:  eyd carr americation a hially one puteicon severlikberevuiyas the force use abohn artwo contmee two at orely sever a united nontion sesen cult on corgisative wi\n",
      "sentence:  qies ors from dethe mark reasun lang maw don he infour three thrigocatespreslyt houte sevelf intow modosdeline eight td one boldna nyarents sox nine six com a d\n",
      "sentence:  vatialgestil mefucwaps the nectik a lifeugus ritm link dhall the s useiouse crcktion im the s deme is seanbed aftermnes seven cent mo debu orde was gon reter th\n",
      "sentence:  ugowinfromlyy be mizs co sarciformation for hoggeny in the phylatileesry be someriarasnoout de colain pemics whey wheneviced ins idestived sugows a next eyer li\n",
      "================================================================================\n",
      "Validation set perplexity: 42.67\n"
     ]
    }
   ],
   "source": [
    "num_steps = 8001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches_bi.next() #train bigram batches\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels)))) #cross-entropy loss\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          '''feed = sample_re(random_distribution())\n",
    "          sentence = characters(feed)[0]'''\n",
    "          feed = sample_re(random_distribution_re(bigram_size))\n",
    "          sentence = bicharacters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed}) # prediction is one-hot encoding (1,729)\n",
    "            feed = sample_re(prediction) #why here using sample_distribution not np.argmax?\n",
    "            sentence += bicharacters(feed)[0]\n",
    "          print('sentence: ',sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches_bi.next()\n",
    "        #print('b: ', b, np.shape(b))\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "import re\n",
    "#import seq2seq_model as s2s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_decoder_input(input_sent, max_size):\n",
    "    sent = input_sent\n",
    "    sentence = ''\n",
    "    sentence_r = ''\n",
    "    for word in sent:\n",
    "        print('word: ', word)\n",
    "        s_word = re.sub('[^a-z]+', '', word.lower()).strip()\n",
    "        print('s_word: ', s_word)\n",
    "        if(len(s_word) == 0):\n",
    "            continue\n",
    "        if(len(sentence) + len(s_word) + 1 > max_size):\n",
    "            break\n",
    "        sentence = sentence + ' ' + s_word.lower()\n",
    "        sentence_r = sentence_r + ' ' + s_word.lower()[::-1]\n",
    "    return sentence.strip(), sentence_r.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('word: ', 'he2l1lo')\n",
      "('s_word: ', 'hello')\n",
      "('word: ', 'how::')\n",
      "('s_word: ', 'how')\n",
      "('word: ', 'ar|e')\n",
      "('s_word: ', 'are')\n",
      "('word: ', 'yo4u')\n",
      "('s_word: ', 'you')\n",
      "('word: ', '7haha')\n",
      "('s_word: ', 'haha')\n",
      "('sent1: ', 'hello how are you haha')\n",
      "('sent2: ', 'olleh woh era uoy ahah')\n"
     ]
    }
   ],
   "source": [
    "max_size = 40\n",
    "input_sent = ['he2l1lo', 'how::','ar|e', 'yo4u', '7haha']\n",
    "sent1, sent2 = create_encoder_decoder_input(input_sent, max_size)\n",
    "print('sent1: ', sent1)\n",
    "print('sent2: ', sent2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
